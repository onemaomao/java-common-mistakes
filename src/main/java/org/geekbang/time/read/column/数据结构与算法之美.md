05 | 数组：为什么很多编程语言中数组都从0开始编号？
如何实现随机访问？
    数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。
    第一是线性表（Linear List）。
        链表、队列、栈等也是线性表结构。
        非线性表，比如二叉树、堆、图等。
    第二个是连续的内存空间和相同类型的数据。
        特别纠正一个“错误”:数组和链表的区别，很多人都回答说，“链表适合插入、删除，时间复杂度 O(1)；数组适合查找，查找时间复杂度为 O(1)”。
        这种表述是不准确的。数组是适合查找操作，但是查找的时间复杂度并不为 O(1)。即便是排好序的数组，你用二分查找，时间复杂度也是 O(logn)。所以，正确的表述应该是，数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。
低效的“插入”和“删除”
    “插入”和“删除”的最好、最坏、平均时间复杂度。
    某些特殊场景下并不一定非得追求数组中数据的连续性。
        示例，JVM 标记清除垃圾回收算法的核心思想。
警惕数组的访问越界问题
    C语言的示例
容器能否完全替代数组？
    ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。它还有一个优势，就是支持动态扩容。
    如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。
    有些时候，用数组会更合适些
        1.Java ArrayList 无法存储基本类型。
        2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
        3.当要表示多维数组时，用数组往往会更加直观。
解答开篇
    数组元素 a【k]】的内存地址的计算公式
    历史原因
内容小结

06 | 链表（上）：如何实现LRU缓存淘汰算法?
存淘汰策略
    先进先出策略 FIFO（First In，First Out）
    最少使用策略 LFU（Least Frequently Used）
    最近最少使用策略 LRU（Least Recently Used）
五花八门的链表结构
    三种最常见的链表结构
        单链表、双向链表和循环链表
单链表
    ->data next->data next->NULL
    插入、删除结点图示，对应的时间复杂度是 O(1)。
    链表要想随机访问第 k 个元素，就没有数组那么高效了。需要 O(n) 的时间复杂度。
循环链表
    循环链表是一种特殊的单链表。
    单链表的尾结点指针指向空地址，循环链表的尾结点指针是指向链表的头结点。
    和单链表相比，循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。
双向链表
    ->pre data next<-->pre data next<-->pre data next
    如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。
    虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。
    双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。
两个操作分析:
    删除操作
        删除结点中“值等于某个给定值”的结点；
            单纯的删除O(1)，但是查找是O(n)
        删除给定指针指向的结点。
            单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p->next=q，说明 p 是 q 的前驱结点。
            单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了。
    插入操作
        如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。
        双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。
    除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。
        因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。
        LinkedHashMap就用到了双向链表这种数据结构。
        用空间换时间的设计思想。
双向循环链表
    自己画图
链表 VS 数组性能大比拼
    插入、删除、随机访问操作的时间复杂度正好相反。
    O(1) O(n)
    ArrayList申请内存与拷贝是很耗时的。
    针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。
解答开篇
    思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
    ......
内容小结
    和数组相比，链表更适合插入、删除操作频繁的场景，查询的时间复杂度较高。不过，在具体软件开发中，要对数组和链表的各种性能进行对比，综合来选择使用两者中的哪一个。

07 | 链表（下）：如何轻松写出正确的链表代码？
技巧一：理解指针或引用的含义
......
技巧二：警惕指针丢失和内存泄漏
......
技巧三：利用哨兵简化实现难度
    哨兵解决“边界问题”的，不直接参与业务逻辑。
    有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。
    带头链表图示
技巧四：重点留意边界条件处理
技巧五：举例画图，辅助思考
技巧六：多写多练，没有捷径
    单链表反转
    链表中环的检测
    两个有序的链表合并
    删除链表倒数第 n 个结点
    求链表的中间结点
内容小结
    写链表代码是最考验逻辑思维能力的。因为，链表代码到处都是指针的操作、边界条件的处理，稍有不慎就容易产生 Bug。

08 | 栈：如何实现浏览器的前进和后退功能？
如何理解“栈”？
    后进者先出，先进者后出，这就是典型的“栈”结构。
    从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。
    当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。
如何实现一个“栈”？
    基于数组实现的示例,数组,元素个数,容量
    时间复杂度、空间复杂度都是O(1)
支持动态扩容的顺序栈
    出栈的时间复杂度仍然是 O(1)
    当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。
    分析，平均情况下的耗时就接近 O(1)。
栈在函数调用中的应用
    操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。
    每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。
栈在表达式求值中的应用
    编译器如何利用栈来实现表达式求值。
    比如：34+13*9+44-12/3。
    实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。
    如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；
    如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。
栈在括号匹配中的应用
    比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。
    用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。
解答开篇
    使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击后退按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y。
    当我们点击前进按钮时，我们依次从栈 Y 中取出数据，放入栈 X 中。
    当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。
    当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了。

09 | 队列：队列在线程池等有限资源池中的应用
如何理解“队列”？
    先进者先出，这就是典型的“队列”。
    栈只支持两个基本操作：入栈 push()和出栈 pop()。
    队列跟栈非常相似，支持的操作也很有限，最基本的操作也是两个：入队 enqueue()，放一个数据到队列尾部；出队 dequeue()，从队列头部取一个元素。
    队列跟栈一样，也是一种操作受限的线性表数据结构。
顺序队列和链式队列
    用数组实现的栈叫作顺序栈，用链表实现的栈叫作链式栈。
    同样，用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。
    数组、容量、队头队尾的下标
循环队列
    原本数组是有头有尾的，是一条直线。现在我们把首尾相连，扳成了一个环。
    要想写出没有 bug 的循环队列的实现代码，最关键的是，确定好队空和队满的判定条件。
    根据画图模拟,当队满时，(tail+1)%n=head。
阻塞队列和并发队列
    阻塞队列其实就是在队列基础上增加了阻塞操作。
        简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；
        如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。
        就是一个“生产者 - 消费者模型”
    线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。
        实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。
解答开篇
    实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。
内容小结
    既可以用数组来实现，也可以用链表来实现。
    用数组实现的叫顺序队列，用链表实现的叫链式队列。 
    特别是长得像一个环的循环队列。在数组实现队列的时候，会有数据搬移操作，要想解决数据搬移的问题，我们就需要像环一样的循环队列。
    循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。

10 | 递归：如何用三行代码找到“最终推荐人”？
如何理解“递归”？
    f(n)=f(n-1)+1 其中，f(1)=1
递归需要满足的三个条件
    1. 一个问题的解可以分解为几个子问题的解
    2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
    3. 存在递归终止条件
如何编写递归代码？
    假如这里有 n 个台阶，每次你可以跨 1 个台阶或者 2 个台阶，请问走这 n 个台阶有多少种走法？如果有 7 个台阶，你可以 2，2，2，1 这样子上去，也可以 1，2，1，1，2 这样子上去，总之走法有很多，那如何用编程求得总共有多少种走法呢？
        可以根据第一步的走法把所有走法分为两类，
        第一类是第一步走了 1 个台阶，另一类是第一步走了 2 个台阶。
        所以 n 个台阶的走法就等于先走 1 阶后，n-1 个台阶的走法 加上先走 2 阶后，n-2 个台阶的走法。
        用公式表示就是：f(n) = f(n-1)+f(n-2)
        ......分析终止条件,终止条件就是 f(1)=1，f(2)=2
        我们把递归终止条件和刚刚得到的递推公式放到一起就是这样的：
            f(1) = 1;
            f(2) = 2;
            f(n) = f(n-1)+f(n-2)
    总结一下，写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。
    编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。
递归代码要警惕堆栈溢出
    可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归调用超过一定深度（比如 1000）之后，我们就不继续往下再递归了，直接返回报错。
递归代码要警惕重复计算
    看示例图
        为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。
        当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。
怎么将递归代码改写为非递归代码？
    在开发过程中，我们要根据实际情况来选择是否需要用递归的方式来实现。
解答开篇
内容小结

11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？
思考题：插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
如何分析一个“排序算法”？
    排序算法的执行效率
        1. 最好情况、最坏情况、平均情况时间复杂度
        2. 时间复杂度的系数、常数 、低阶
        3. 比较次数和交换（或移动）次数
    排序算法的内存消耗
        原地排序（Sorted in place）特指空间复杂度是 O(1) 的排序算法
    排序算法的稳定性
        见文稿描述
冒泡排序（Bubble Sort）
    第一，冒泡排序是原地排序算法吗？
        空间复杂度为 O(1)，是一个原地排序算法
    第二，冒泡排序是稳定的排序算法吗？
        有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。
    第三，冒泡排序的时间复杂度是多少？
        最好情况时间复杂度是 O(n),所以最坏情况时间复杂度为 O(n2),平均情况下的时间复杂度就是 O(n2)
插入排序（Insertion Sort）
    第一，插入排序是原地排序算法吗？
        插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。
    第二，插入排序是稳定的排序算法吗？
        对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。
    第三，插入排序的时间复杂度是多少？
        最好是时间复杂度为 O(n),最坏情况时间复杂度为 O(n2),平均时间复杂度为 O(n2)
选择排序（Selection Sort）
    选择排序是一种不稳定的排序算法
解答开篇
    见文稿的对比内容
内容小结
    三种时间复杂度是 O(n2) 的排序算法，冒泡排序、插入排序、选择排序。

12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？
两种时间复杂度为 O(nlogn) 的排序算法，归并排序和快速排序。
归并排序的原理
    如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。
    归并排序使用的就是分治思想。分治，顾名思义，就是分而治之，将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了。
    分治是一种解决问题的处理思想，递归是一种编程技巧
    过程图见文稿
归并排序的性能分析
    第一，归并排序是稳定的排序算法吗？
        合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。
    第二，归并排序的时间复杂度是多少？
        不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。
    第三，归并排序的空间复杂度是多少？
        归并排序不是原地排序算法。
        空间复杂度是 O(n)
快速排序的原理
    快排利用的也是分治思想。
    过程图见文稿
    归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。
    归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法。
    归并之所以是非原地排序算法，主要原因是合并函数无法在原地执行。快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题。
快速排序的性能分析
    快排的时间复杂度也是 O(nlogn)。
    T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。
内容小结
    理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。
    快速排序算法时间复杂度退化到 O(n2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。

13 | 线性排序：如何根据年龄给100万用户数据排序？
桶排序（Bucket sort）
    时间复杂度是 O(n)
    桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？
        首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序。
        其次，数据在各个桶之间的分布是比较均匀的。
        桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
计数排序（Counting sort）
    计数排序其实是桶排序的一种特殊情况
    为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？
    计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。
基数排序（Radix sort）
    基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

14 | 排序优化：如何实现一个通用的、高性能的排序函数？
如何选择合适的排序算法？
    排序算法的对比表格
    选择与对比
        小规模数据进行排序，可以选择时间复杂度是 O(n2) 的算法；
        如果对大规模数据进行排序，时间复杂度是 O(nlogn) 的算法更加高效。
        为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 O(nlogn) 的排序算法来实现排序函数。
        堆排序和快速排序都有比较多的应用，比如 Java 语言采用堆排序实现排序函数，C 语言使用快速排序实现排序函数。
        使用归并排序的情况其实并不多
            归并排序并不是原地排序算法，空间复杂度是 O(n)
        快速排序在最坏情况下的时间复杂度是 O(n2)
如何优化快速排序？
    O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理。
    最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多。
    两个比较常用、比较简单的分区算法
        1. 三数取中法
            从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。
        2. 随机法
            随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。
举例分析排序函数
    C 语言的 qsort()
内容小结

15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？
无处不在的二分思想
    猜字游戏二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。
O(logn) 惊人的查找速度
    用大 O 标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。对于常量级时间复杂度的算法来说，O(1) 有可能表示的是一个非常大的常量值，比如 O(1000)、O(10000)。所以，常量级时间复杂度的算法有时候可能还没有 O(logn) 的算法执行效率高。
二分查找的递归与非递归实现
    最简单的情况就是有序数组中不存在重复元素
    low、high、mid 都是指数组下标，其中 low 和 high 表示当前查找的区间范围，初始 low=0， high=n-1。mid 表示 [low, high] 的中间位置。
    1. 循环退出条件
       注意是 low<=high，而不是 low<high。 
    2.mid 的取值
        low+((high-low)>>1)
    3.low 和 high 的更新
        low=mid+1，high=mid-1。
    实际上，二分查找除了用循环来实现，还可以用递归来实现，过程也非常简单。
二分查找应用场景的局限性
    首先，二分查找依赖的是顺序表结构，简单点说就是数组。
    其次，二分查找针对的是有序数据。
    再次，数据量太小不适合二分查找。
    最后，数据量太大也不适合二分查找。
        数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。

16 | 二分查找（下）：如何快速定位IP对应的省份地址？
变体一：查找第一个值等于给定值的元素
    文稿示例
变体二：查找最后一个值等于给定值的元素
变体三：查找第一个大于等于给定值的元素
变体四：查找最后一个小于等于给定值的元素
解答开篇
内容小结

17 | 跳表：为什么Redis一定要用跳表来实现有序集合？
如何理解“跳表”？
    加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。
    链表加多级索引的结构，就是跳表。
用跳表查询到底有多快？
    在跳表中查询任意数据的时间复杂度就是 O(logn)
跳表是不是很浪费内存？
    跳表的空间复杂度分析并不难，假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。
    这几级索引的结点总和就是 n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是 O(n)
高效的动态插入和删除
    跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。
    如何在跳表中插入一个数据，以及它是如何做到 O(logn) 的时间复杂度的？
        查找某个结点的的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。
跳表索引动态更新
    当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。
    当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。
        通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。
解答开篇
    详细阅读文稿
    Redis 中的有序集合支持的核心操作主要有下面这几个：
        插入一个数据；
        删除一个数据；
        查找一个数据；
        按照区间查找数据（比如查找值在 [100, 356] 之间的数据）；
        迭代输出有序序列。
    插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。
    对于按照区间查找数据这个操作，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了。这样做非常高效。
    Redis 之所以用跳表来实现有序集合，还有其他原因，比如，跳表更容易代码实现。虽然跳表的实现也不简单，但比起红黑树来说还是好懂、好写多了，而简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。
    不过，跳表也不能完全替代红黑树。因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。

18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？
散列思想
    散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。
散列函数
    三点散列函数设计的基本要求：
        散列函数计算得到的散列值是一个非负整数；
        如果 key1 = key2，那 hash(key1) == hash(key2)；
        如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。
        第三点，即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突。而且，因为数组的存储空间有限，也会加大散列冲突的概率。
散列冲突
    1. 开放寻址法
       核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。
       如何重新探测新的位置
            线性探测（Linear Probing）。
                对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。
                在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。
                    我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。
                线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。
            二次探测（Quadratic probing）和双重散列（Double hashing）。
                所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+12，hash(key)+22……
                所谓双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。
       不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。
       装载因子的计算公式是：
            散列表的装载因子 = 填入表中的元素个数 / 散列表的长度
    2. 链表法
       链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。 
解答开篇
    常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
    当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。

19 | 散列表（中）：如何打造一个工业级水平的散列表？
如何设计散列函数？
    散列函数的设计不能太复杂。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能。
    其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。
装载因子过大了怎么办？
    装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。
    不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。
    针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中。
        针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。
    插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。
    实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。
    我们前面讲到，当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。
    装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。
如何避免低效地扩容？
    当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。
    当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。
    这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。
如何选择冲突解决方法？
    1. 开放寻址法
       当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
    2. 链表法
       基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。
工业级散列表举例分析-Java 中的 HashMap
    1. 初始大小
    2. 装载因子和动态扩容
    3. 散列冲突解决方法
    4. 散列函数
解答开篇
何为一个工业级的散列表？工业级的散列表应该具有哪些特性？
    支持快速的查询、插入、删除操作；
    内存占用合理，不能浪费过多的内存空间；
    性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。
如何实现这样一个散列表呢？
    设计一个合适的散列函数；
    定义装载因子阈值，并且设计动态扩容策略；
    选择合适的散列冲突解决方法。

20 | 散列表（下）：为什么散列表和链表经常会一起使用？
LRU 缓存淘汰算法
    散列表+双向链表实现
        如何查找一个数据
            散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。
        如何删除一个数据
            需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在 O(1) 时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。
        如何添加一个数据
            先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；
            如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；
            如果没有满，就直接将数据放到链表的尾部。
Redis 有序集合
    ......
Java LinkedHashMap
    按照访问时间排序的 LinkedHashMap 本身就是一个支持 LRU 缓存淘汰策略的缓存系统？实际上，它们两个的实现原理也是一模一样的。
    实际上，LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。
解答开篇 & 内容小结
    散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。
    因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？
什么是哈希算法？
    将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。
    总结了需要满足的几点要求：
        从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
        对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
        散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
        哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。
应用一：安全加密
    MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）
    SHA（Secure Hash Algorithm，安全散列算法）
    DES（Data Encryption Standard，数据加密标准）
    AES（Advanced Encryption Standard，高级加密标准）
    为什么哈希算法无法做到零冲突？
        鸽巢原理（也叫抽屉原理）
应用二：唯一标识
    海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。
应用三：数据校验
    BT 下载软件
        对 100 个文件块分别取哈希值，并且保存在种子文件中。
        当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。
应用四：散列函数
    相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。
    不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。
    除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。
解答开篇

22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？
应用五：负载均衡
    负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？
    可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。
应用六：数据分片
    1. 如何统计“搜索关键词”出现的次数？
       我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。
       具体的思路是这样的：为了提高处理的速度，我们用 n 台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。
    2. 如何快速判断图片是否在图库中？
        文稿示例，图片量过亿采用单台机器构建散列表不合适。
应用七：分布式存储
    一致性哈希算法
解答开篇 & 内容小结

23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？
树（Tree）
    三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）
    见图示
二叉树（Binary Tree）
    叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。
    叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。
        见图示
    如何表示（或者存储）一棵二叉树？
        一种是基于指针或者引用的二叉链式存储法
            见图示。大部分二叉树代码都是通过这种结构来实现的。
        一种是基于数组的顺序存储法
            见图示。根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置，以此类推。
            如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
二叉树的遍历
    前序遍历、中序遍历和后序遍历。
    实际上，二叉树的前、中、后序遍历就是一个递归的过程。
    二叉树遍历的时间复杂度是多少
        O(n)
解答开篇 & 内容小结

24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？
二叉查找树（Binary Search Tree）
    二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。
    1. 二叉查找树的查找操作
    2. 二叉查找树的插入操作
    3. 二叉查找树的删除操作
        如果要删除的节点没有子节点
        如果要删除的节点只有一个子节点（只有左子节点或者右子节点）
        如果要删除的节点有两个子节点
            关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。
    4. 二叉查找树的其他操作
        快速地查找最大节点和最小节点、前驱节点和后继节点。
        中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。
支持重复数据的二叉查找树
二叉查找树的时间复杂度分析
    不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 O(height)。
    完全二叉树的高度小于等于 log2n

25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？
什么是“平衡二叉查找树”？
    平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。
    平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。
    所以，如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 log2n 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树。
如何定义一棵“红黑树”？
    红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：
        根节点是黑色的；
        每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
        任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
        每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；
为什么说红黑树是“近似平衡”的？
    “平衡”的意思可以等价为性能不退化。“近似平衡”就等价为性能不会退化的太严重。
内容小结
    红黑树的高度近似 log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。
    因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。

26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树
实现红黑树的基本思想
    红黑树的平衡过程跟魔方复原非常神似，大致过程就是：遇到什么样的节点排布，我们就对应怎么去调整。
    两个非常重要的操作，左旋（rotate left）、右旋（rotate right）。左旋全称其实是叫围绕某个节点的左旋，那右旋的全称就叫围绕某个节点的右旋。
插入操作的平衡调整
    红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。
        如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。
        如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。
        除此之外，其他情况都会违背红黑树的定义，于是我们就需要进行调整，调整的过程包含两种基础的操作：左右旋转和改变颜色。
删除操作的平衡调整
1. 针对删除节点初步调整
2. 针对关注节点进行二次调整





















